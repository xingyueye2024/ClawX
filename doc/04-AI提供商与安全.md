# AI 提供商与安全

## 1. 支持的 AI 提供商

ClawX 支持丰富的 AI 提供商，覆盖国际主流与国内厂商：

### 国际提供商

| 提供商 | 模型系列 | API Key | 基础 URL | 说明 |
|--------|----------|---------|----------|------|
| **Anthropic** | Claude | 必需 | 默认 | Anthropic 官方 Claude API |
| **OpenAI** | GPT | 必需 | 默认 | OpenAI 官方 API |
| **Google** | Gemini | 必需 | 默认 | Google AI Studio |
| **OpenRouter** | 多模型 | 必需 | 默认 | 模型聚合服务，一个 Key 调用多家模型 |

### 国内提供商

| 提供商 | 模型系列 | API Key | 基础 URL | 说明 |
|--------|----------|---------|----------|------|
| **Moonshot** | Kimi | 必需 | 自定义 | 月之暗面（Kimi）大模型 |
| **SiliconFlow** | 多模型 | 必需 | 自定义 | 硅基流动，支持多种开源模型 |

### 本地与自定义

| 提供商 | 模型系列 | API Key | 基础 URL | 说明 |
|--------|----------|---------|----------|------|
| **Ollama** | 多种本地模型 | 可选 | 可配置 | 本地模型部署，支持 Llama、Mistral 等 |
| **自定义** | 任意 | 必需 | 可配置 | 用户自定义 API 端点 |

### 提供商功能

- **多实例支持** — 同一提供商可配置多个实例（如不同 API Key）
- **默认提供商选择** — 可指定一个提供商作为默认聊天使用
- **Key 验证** — 保存前自动验证 API Key 的有效性
- **环境变量注入** — API Key 自动注入为 Gateway 进程的环境变量

---

## 2. 安全机制

### 2.1 API Key 安全存储

ClawX 使用系统原生钥匙串（Keychain）存储 API Key，确保敏感凭据安全：

- **macOS** — 使用 macOS Keychain 服务
- **Windows** — 使用 Windows 凭据管理器（DPAPI）
- **Linux** — 使用 libsecret / GNOME Keyring

技术实现：
- 通过 Electron 的 `safeStorage` API 加密存储
- 使用 `electron-store` 持久化加密后的数据
- API Key 仅在主进程中解密，不会暴露给渲染进程

### 2.2 进程隔离

ClawX 严格遵循 Electron 安全最佳实践：

| 安全措施 | 状态 | 说明 |
|----------|------|------|
| `contextIsolation` | 已启用 | 主进程与渲染进程上下文完全隔离 |
| `nodeIntegration` | 已禁用 | 渲染进程不可直接访问 Node.js API |
| `sandbox` | 已启用 | Preload 脚本在沙箱中运行 |
| IPC 通道 | 白名单 | 仅暴露必要的 IPC 调用 |

### 2.3 Gateway 认证

- Gateway 进程使用 Token 认证机制
- Token 在主进程中生成并安全传递
- WebSocket 连接通过 Token 验证身份
- 外部连接也需提供有效 Token

### 2.4 外部链接安全

- 所有外部链接通过 `shell.openExternal` 在系统默认浏览器中打开
- 渲染进程不直接处理外部 URL
- 防止通过渲染进程执行恶意链接

---

## 3. 提供商配置指南

### 3.1 配置步骤

1. 进入 **设置 → AI 提供商** 页面
2. 点击要配置的提供商卡片
3. 输入 API Key
4. （可选）修改基础 URL（自定义提供商）
5. 点击保存，系统自动验证 Key 有效性
6. 验证通过后，Key 被加密存储到系统钥匙串

### 3.2 默认提供商

- 可在提供商列表中设定一个默认提供商
- 聊天界面默认使用该提供商
- 用户可在聊天会话中按需切换不同提供商

### 3.3 Ollama 本地模型

对于希望完全本地运行的用户：
1. 在本机安装 Ollama
2. 在 ClawX 中配置 Ollama 提供商
3. 设置 Ollama 服务器地址（默认 `http://localhost:11434`）
4. 选择已下载的本地模型
5. 所有数据在本地处理，无需联网
